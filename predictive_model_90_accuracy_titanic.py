{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:39.014107Z\",\"iopub.execute_input\":\"2022-11-26T22:05:39.014570Z\",\"iopub.status.idle\":\"2022-11-26T22:05:41.875477Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:39.014496Z\",\"shell.execute_reply\":\"2022-11-26T22:05:41.874662Z\"}}\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport plotly\nimport re\n          \nplotly.offline.init_notebook_mode() # run at the start of every notebook\nimport cufflinks as cf\n\ncf.go_offline()\ncf.getThemes()\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n%matplotlib inline\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom IPython.display import display\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:41.876495Z\",\"iopub.execute_input\":\"2022-11-26T22:05:41.876948Z\",\"iopub.status.idle\":\"2022-11-26T22:05:41.917963Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:41.876885Z\",\"shell.execute_reply\":\"2022-11-26T22:05:41.917123Z\"}}\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\nfull_data = [df_train,df_test]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:41.919156Z\",\"iopub.execute_input\":\"2022-11-26T22:05:41.919441Z\",\"iopub.status.idle\":\"2022-11-26T22:05:41.935715Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:41.919383Z\",\"shell.execute_reply\":\"2022-11-26T22:05:41.934925Z\"}}\ndf_train.info()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:41.937011Z\",\"iopub.execute_input\":\"2022-11-26T22:05:41.937421Z\",\"iopub.status.idle\":\"2022-11-26T22:05:41.952796Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:41.937358Z\",\"shell.execute_reply\":\"2022-11-26T22:05:41.951967Z\"}}\n# Function to calculate no. of null values with percentage in the dataframe\ndef null_values(DataFrame_Name):\n    \n    sum_null = DataFrame_Name.isnull().sum()\n    total_count = DataFrame_Name.isnull().count()\n    percent_nullvalues = sum_null/total_count * 100\n    df_null = pd.DataFrame()\n    df_null['Total_values'] = total_count\n    df_null['Null_Count'] = sum_null\n    df_null['Percent'] = percent_nullvalues\n    df_null = df_null.sort_values(by='Null_Count',ascending = False)\n\n    return(df_null)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:41.954000Z\",\"iopub.execute_input\":\"2022-11-26T22:05:41.954462Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.030400Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:41.954406Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.029568Z\"}}\nnull_values(df_train)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.031746Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.032234Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.054141Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.032088Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.053346Z\"}}\nnull_values(df_test)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.055357Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.055700Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.109677Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.055647Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.108966Z\"}}\ndf_train.describe()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.110747Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.111120Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.143891Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.111068Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.143213Z\"}}\ndf_train.head(5)\n\n# %% [markdown]\n# # Correlation\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.144915Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.145193Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.157223Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.145139Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.156370Z\"}}\n## get the most important variables. \ncorr = df_train.corr()**2\ncorr.Survived.sort_values(ascending=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.158668Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.159108Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.821288Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.158913Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.820062Z\"}}\n## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(df_train.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu_r',\n            linewidths=0.1, \n            linecolor='white',\n            vmax = .9,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);\n\n# %% [markdown]\n# # Pclass\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.823231Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.823978Z\",\"iopub.status.idle\":\"2022-11-26T22:05:42.845391Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.823892Z\",\"shell.execute_reply\":\"2022-11-26T22:05:42.844777Z\"}}\n# Lets start with Pclass column - Already an integer - good\n# Lets check the impact of this column on the survived column in the train dataset.\n# We will calculate mean of survived people in each class - This will tell us how many survived out of total for each class\ndf_train[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean()\n\n# %% [markdown]\n# From the above calculation, it seems like passenger from class1 ( mostly rich ) survived with a maximum percentage\n# and passengers from a lower class survived least\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:42.846282Z\",\"iopub.execute_input\":\"2022-11-26T22:05:42.846618Z\",\"iopub.status.idle\":\"2022-11-26T22:05:43.107546Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:42.846578Z\",\"shell.execute_reply\":\"2022-11-26T22:05:43.106229Z\"}}\nsns.barplot('Pclass','Survived', data=df_train)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:43.109876Z\",\"iopub.execute_input\":\"2022-11-26T22:05:43.110617Z\",\"iopub.status.idle\":\"2022-11-26T22:05:43.521328Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:43.110286Z\",\"shell.execute_reply\":\"2022-11-26T22:05:43.520531Z\"}}\n# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(df_train.Pclass[df_train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(df_train.loc[(df_train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived')\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Passenger Class\", fontsize = 15)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(df_train.Pclass.unique()), labels);\n\n# %% [markdown]\n# # Sex\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:43.522348Z\",\"iopub.execute_input\":\"2022-11-26T22:05:43.522629Z\",\"iopub.status.idle\":\"2022-11-26T22:05:43.530571Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:43.522582Z\",\"shell.execute_reply\":\"2022-11-26T22:05:43.529752Z\"}}\nfemales=df_train['Sex'].apply(lambda x: x.count('female')).sum()\nprint('Total males=',891-females)\nprint('Total females=',females)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:43.531629Z\",\"iopub.execute_input\":\"2022-11-26T22:05:43.531919Z\",\"iopub.status.idle\":\"2022-11-26T22:05:43.552422Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:43.531872Z\",\"shell.execute_reply\":\"2022-11-26T22:05:43.551712Z\"}}\n# Now lets focus on the Sex column and evaluate its impact on the survived column\ndf_train[['Sex','Survived']].groupby(['Sex'],as_index = False).mean()\n\n# %% [markdown]\n# From the above calculation it is clear that the female survival rate is much higher than the male survivor rate\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:43.553581Z\",\"iopub.execute_input\":\"2022-11-26T22:05:43.554067Z\",\"iopub.status.idle\":\"2022-11-26T22:05:43.797588Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:43.554004Z\",\"shell.execute_reply\":\"2022-11-26T22:05:43.796188Z\"}}\nsns.barplot(x='Sex', y='Survived', data=df_train)\n\n# %% [markdown]\n# # Embarked\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:43.799663Z\",\"iopub.execute_input\":\"2022-11-26T22:05:43.800522Z\",\"iopub.status.idle\":\"2022-11-26T22:05:43.833229Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:43.800416Z\",\"shell.execute_reply\":\"2022-11-26T22:05:43.832071Z\"}}\ndf_train[['Embarked','Survived']].groupby(['Embarked'],as_index = False).mean()\n\n# %% [markdown]\n# People with destination C = Cherbourg (C = Cherbourg, Q = Queenstown, S = Southampton) survived with highest percentage\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:43.835180Z\",\"iopub.execute_input\":\"2022-11-26T22:05:43.835886Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.097554Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:43.835776Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.096365Z\"}}\nsns.barplot(x='Embarked', y='Survived', data=df_train)\n\n# %% [markdown]\n# # SibSp & Parch = Family_members\n\n# %% [markdown]\n# Lets create a new feature column by combining sibling/spouse & parent/children column\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.099505Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.100262Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.140222Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.100157Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.138949Z\"}}\ndf_train['Family_members'] = df_train['SibSp'] + df_train['Parch']\ndf_test['Family_members'] = df_test['SibSp'] + df_test['Parch']\ndf_train[['Family_members','Survived']].groupby(['Family_members'],as_index=False).mean()\n\n\n# %% [markdown]\n# From the above calculation, we can conclude that - Survival percentage is higher when Family members are #1,2,3\n# It is less when you are alone or have family members > 3\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.142045Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.142770Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.528549Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.142686Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.527270Z\"}}\nsns.barplot(x='Family_members', y='Survived', data=df_train)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.530448Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.531226Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.540305Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.531113Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.539056Z\"}}\ndf_train = df_train.drop(['PassengerId'],axis=1)\n#df_test = df_test.drop(['PassengerId'],axis=1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.542154Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.542914Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.552315Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.542802Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.551062Z\"}}\nfull_data = [df_train,df_test]\n\n\n# %% [markdown]\n# # Lets focus on the Missing Values\n# Cabin - Removing this column from the dataset- 80% missing values\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.554221Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.554921Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.569550Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.554808Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.568320Z\"}}\ndf_train = df_train.drop(['Cabin','Ticket'],axis=1)\n\ndf_test = df_test.drop(['Cabin','Ticket'],axis=1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.571405Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.572091Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.582182Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.572001Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.580816Z\"}}\nfull_data = [df_train,df_test]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.583698Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.584288Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.619316Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.583943Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.618464Z\"}}\nfor dataset in full_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(df_train['Title'], df_train['Sex'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.620567Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.620931Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.656469Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.620790Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.655637Z\"}}\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.657613Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.657958Z\",\"iopub.status.idle\":\"2022-11-26T22:05:44.948176Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.657863Z\",\"shell.execute_reply\":\"2022-11-26T22:05:44.946880Z\"}}\nsns.barplot(x='Title', y='Survived', data=df_train)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:44.950106Z\",\"iopub.execute_input\":\"2022-11-26T22:05:44.950910Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.041131Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:44.950741Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.040371Z\"}}\ndf_train\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.042332Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.042616Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.047999Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.042558Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.047180Z\"}}\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,LabelBinarizer\n#cat_features = df_train['Title']\n#encoder = LabelBinarizer()\n#new_cat_features = encoder.fit_transform(cat_features)\n#new_cat_features\n\n#pd.get_dummies(df_train, columns=['Title'], prefix=['Title'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.049186Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.049475Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.061928Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.049417Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.061140Z\"}}\ndf_train = df_train.drop(['Name'],axis = 1)\ndf_test = df_test.drop(['Name'],axis = 1)\n\n\n# %% [markdown]\n# # Age\n# We will find the average age for every category in the age column and then impute the mean value for the respective category\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.063046Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.063329Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.086890Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.063272Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.086090Z\"}}\n\ndf_train[['Title','Age']].groupby(['Title'],as_index = False).mean().sort_values(by='Age')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.088482Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.088908Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.286739Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.088817Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.285389Z\"}}\nMean_Age = df_train[['Title','Age']].groupby(['Title'],as_index = False).mean().sort_values(by='Age')\nsns.barplot(x='Title', y='Age', data=Mean_Age)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.289264Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.290183Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.303000Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.289709Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.301559Z\"}}\ndf_train['Age'] = df_train['Age'].fillna(-1)\ndf_test['Age'] = df_test['Age'].fillna(-1)  \nfull_data = [df_train,df_test]\n\n# %% [markdown]\n# # Age\n# Null Values - 20% - Imputing the mean value per category as calculated above\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.305284Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.306110Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.384116Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.305713Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.383441Z\"}}\n\nfor dataset in full_data:\n    \n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Master'), 'Age'] = 4.57\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Miss'), 'Age'] = 21.84\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Mr'), 'Age'] = 32.36\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Mrs'), 'Age'] = 35.78\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Rare'), 'Age'] = 45.54\n    dataset['Age'] = dataset['Age'].astype(int)   \n    \n\n# %% [markdown]\n# Now creating different age bands...\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.385330Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.385790Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.455877Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.385741Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.455106Z\"}}\nfull_data = [df_train, df_test]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 7\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.457065Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.457324Z\",\"iopub.status.idle\":\"2022-11-26T22:05:45.484705Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.457277Z\",\"shell.execute_reply\":\"2022-11-26T22:05:45.483930Z\"}}\ndf_train[['Sex','Age','Survived']].groupby(['Sex','Age'],as_index=False).mean()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:45.485809Z\",\"iopub.execute_input\":\"2022-11-26T22:05:45.486120Z\",\"iopub.status.idle\":\"2022-11-26T22:05:46.085380Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:45.486055Z\",\"shell.execute_reply\":\"2022-11-26T22:05:46.084196Z\"}}\nagesexsurv = df_train[['Sex','Age','Survived']].groupby(['Sex','Age'],as_index=False).mean()\nsns.factorplot('Age','Survived','Sex', data=agesexsurv\n                ,aspect=3,kind='bar')\nplt.suptitle('AgeBand,Sex vs Survived')\n\n# %% [markdown]\n# # I want to create different categories for family members\n# as calculated above\n# Family_members vs Survived\n# \n# \n# category 0 = person is alone - survival chance = 30%,\n# category 1 = person has family members = 1,2 - survival chance = 56%,\n# category 2 = person has family members = 3 - survival chance = 72%\n# category 3 = person has family members = 4,5 - survival chance = approx 17%,\n# category 4 = person has family members = 6 - survival chance = 33%\n# category 5 = person has family members = 7,10 - survival chance = 0%\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:46.087333Z\",\"iopub.execute_input\":\"2022-11-26T22:05:46.088068Z\",\"iopub.status.idle\":\"2022-11-26T22:05:46.145393Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:46.087985Z\",\"shell.execute_reply\":\"2022-11-26T22:05:46.144741Z\"}}\nfull_data = [df_train, df_test]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Family_members'] == 0, 'Family_members_Band'] = 0\n    dataset.loc[(dataset['Family_members'] == 1)|(dataset['Family_members'] == 2),'Family_members_Band'] = 1\n    dataset.loc[ dataset['Family_members'] == 3, 'Family_members_Band'] = 2\n    dataset.loc[(dataset['Family_members'] == 4)|(dataset['Family_members'] == 5),'Family_members_Band'] = 3\n    dataset.loc[ dataset['Family_members'] == 6, 'Family_members_Band'] = 4\n    dataset.loc[(dataset['Family_members'] == 7)|(dataset['Family_members'] == 10),'Family_members_Band'] = 5\n    dataset['Family_members_Band'] = dataset['Family_members_Band'].astype(int)\n\n# %% [markdown]\n# # Creating Categories for Fare column\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:46.146322Z\",\"iopub.execute_input\":\"2022-11-26T22:05:46.146680Z\",\"iopub.status.idle\":\"2022-11-26T22:05:46.185544Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:46.146626Z\",\"shell.execute_reply\":\"2022-11-26T22:05:46.184566Z\"}}\ndf_train['FareBand'] = pd.qcut(df_train['Fare'], 4)\ndf_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:46.186961Z\",\"iopub.execute_input\":\"2022-11-26T22:05:46.187338Z\",\"iopub.status.idle\":\"2022-11-26T22:05:46.416563Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:46.187270Z\",\"shell.execute_reply\":\"2022-11-26T22:05:46.415196Z\"}}\nFarePlot = df_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand')\nsns.barplot(x='FareBand', y='Survived', data=FarePlot)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:46.418564Z\",\"iopub.execute_input\":\"2022-11-26T22:05:46.419299Z\",\"iopub.status.idle\":\"2022-11-26T22:05:46.430938Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:46.419216Z\",\"shell.execute_reply\":\"2022-11-26T22:05:46.429465Z\"}}\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].dropna().mean()) # df_test has one null value\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:46.433010Z\",\"iopub.execute_input\":\"2022-11-26T22:05:46.439186Z\",\"iopub.status.idle\":\"2022-11-26T22:05:46.494536Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:46.439034Z\",\"shell.execute_reply\":\"2022-11-26T22:05:46.493782Z\"}}\nfull_data = [df_train,df_test]\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare_Band'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare_Band'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare_Band'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare_Band'] = 3\n    dataset['Fare_Band'] = dataset['Fare_Band'].astype(int)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:46.495705Z\",\"iopub.execute_input\":\"2022-11-26T22:05:46.496017Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.041481Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:46.495957Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.040564Z\"}}\nsns.factorplot('Fare_Band','Survived','Sex', data=df_train\n                ,aspect=3,kind='bar')\nplt.suptitle('FareBand,Sex vs Survived')\n\n# %% [markdown]\n# From the above graph, thing to notice is that: the males survival rate increases as the fare for the ticket increases but for the females, the survival rate is almost similar for all the fare bands\n\n# %% [markdown]\n# # Embarked Column\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.042798Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.043303Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.051691Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.043240Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.050708Z\"}}\nmost_frequent = df_train['Embarked'].mode()[0]\nmost_frequent\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.053129Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.053702Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.078764Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.053440Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.077915Z\"}}\nfull_data = [df_train,df_test]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(most_frequent)\n    \ndf_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.080006Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.080490Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.384553Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.080428Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.383667Z\"}}\nembarkedgraph = df_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\nsns.barplot(x='Embarked',y='Survived',data=embarkedgraph)\n\n\n# %% [markdown]\n# Dropping Values\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.385957Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.386596Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.394397Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.386530Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.393393Z\"}}\ndf_train = df_train.drop(['SibSp','Parch','Fare','Family_members','FareBand'],axis = 1)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.396466Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.397430Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.404875Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.397357Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.403967Z\"}}\ndf_test = df_test.drop(['SibSp','Parch','Fare','Family_members'],axis = 1)\n\n\n# %% [markdown]\n# # One Hot Encoding\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.406331Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.406798Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.424800Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.406615Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.424029Z\"}}\nX_train = pd.get_dummies(df_train, columns=['Pclass','Sex','Age','Embarked','Title','Family_members_Band','Fare_Band'], prefix=['Pclass','Sex'\n                                                                ,'Age','Embarked','Title','Family_members_Band','Fare_Band'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.426451Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.426818Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.435747Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.426738Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.434993Z\"}}\nY_train = X_train['Survived']\nX_train = X_train.drop('Survived', axis=1)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.437419Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.438151Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.444988Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.438082Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.444033Z\"}}\nX_train.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.446656Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.447331Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.465922Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.447240Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.464888Z\"}}\nX_test = pd.get_dummies(df_test, columns=['Pclass','Sex','Age','Embarked','Title','Family_members_Band','Fare_Band'], prefix=['Pclass','Sex','Age','Embarked','Title','Family_members_Band','Fare_Band'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.468859Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.469527Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.476226Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.469451Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.475372Z\"}}\nX_test.shape\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.477881Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.478535Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.486022Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.478466Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.485209Z\"}}\nX_test=X_test.drop(['PassengerId'],axis=1)\n\n# %% [markdown]\n# # Testing Machine Learning Models\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.487453Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.488024Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.533302Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.487943Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.532105Z\"}}\n# stochastic gradient descent (SGD) learning\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n\nprint(round(acc_sgd,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.535436Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.536295Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.839976Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.536203Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.839214Z\"}}\n# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.841249Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.841524Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.861123Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.841467Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.859927Z\"}}\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.863000Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.863700Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.980192Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.863597Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.979356Z\"}}\n# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.981454Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.981746Z\",\"iopub.status.idle\":\"2022-11-26T22:05:47.995615Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.981687Z\",\"shell.execute_reply\":\"2022-11-26T22:05:47.994581Z\"}}\n# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:47.996999Z\",\"iopub.execute_input\":\"2022-11-26T22:05:47.997410Z\",\"iopub.status.idle\":\"2022-11-26T22:05:48.015449Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:47.997349Z\",\"shell.execute_reply\":\"2022-11-26T22:05:48.014534Z\"}}\n# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(round(acc_perceptron,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:48.016639Z\",\"iopub.execute_input\":\"2022-11-26T22:05:48.016950Z\",\"iopub.status.idle\":\"2022-11-26T22:05:48.066918Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:48.016885Z\",\"shell.execute_reply\":\"2022-11-26T22:05:48.065708Z\"}}\n# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:48.068757Z\",\"iopub.execute_input\":\"2022-11-26T22:05:48.069462Z\",\"iopub.status.idle\":\"2022-11-26T22:05:48.095537Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:48.069372Z\",\"shell.execute_reply\":\"2022-11-26T22:05:48.094340Z\"}}\n# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:48.097480Z\",\"iopub.execute_input\":\"2022-11-26T22:05:48.098272Z\",\"iopub.status.idle\":\"2022-11-26T22:05:48.145250Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:48.098177Z\",\"shell.execute_reply\":\"2022-11-26T22:05:48.143720Z\"}}\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\n\nresult_df.head(9)\n\n# %% [markdown]\n# # Best Model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:48.152930Z\",\"iopub.execute_input\":\"2022-11-26T22:05:48.156951Z\",\"iopub.status.idle\":\"2022-11-26T22:05:48.545431Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:48.156800Z\",\"shell.execute_reply\":\"2022-11-26T22:05:48.544090Z\"}}\nbestmodelgraph = result_df.head(9)\nax = sns.factorplot(\"Model\", y=\"Score\", data=bestmodelgraph,\n                palette='Blues_d',aspect=3.5,kind='bar')\n\n# %% [markdown]\n# # K-FOLD Validation\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:48.547439Z\",\"iopub.execute_input\":\"2022-11-26T22:05:48.548206Z\",\"iopub.status.idle\":\"2022-11-26T22:05:49.958442Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:48.548113Z\",\"shell.execute_reply\":\"2022-11-26T22:05:49.957683Z\"}}\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:49.959389Z\",\"iopub.execute_input\":\"2022-11-26T22:05:49.959744Z\",\"iopub.status.idle\":\"2022-11-26T22:05:49.967270Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:49.959701Z\",\"shell.execute_reply\":\"2022-11-26T22:05:49.966503Z\"}}\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:49.968625Z\",\"iopub.execute_input\":\"2022-11-26T22:05:49.969004Z\",\"iopub.status.idle\":\"2022-11-26T22:05:50.007401Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:49.968935Z\",\"shell.execute_reply\":\"2022-11-26T22:05:50.006369Z\"}}\n# Plot learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:50.009039Z\",\"iopub.execute_input\":\"2022-11-26T22:05:50.009391Z\",\"iopub.status.idle\":\"2022-11-26T22:05:56.925179Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:50.009326Z\",\"shell.execute_reply\":\"2022-11-26T22:05:56.923955Z\"}}\n# Plot learning curves\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\ntitle = \"Learning Curves (Random Forest)\"\ncv = 10\nplot_learning_curve(rf, title, X_train, Y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=1);\n\n# %% [markdown]\n# # Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:56.927126Z\",\"iopub.execute_input\":\"2022-11-26T22:05:56.927806Z\",\"iopub.status.idle\":\"2022-11-26T22:05:56.951411Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:56.927722Z\",\"shell.execute_reply\":\"2022-11-26T22:05:56.950165Z\"}}\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:56.953296Z\",\"iopub.execute_input\":\"2022-11-26T22:05:56.953995Z\",\"iopub.status.idle\":\"2022-11-26T22:05:57.328978Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:56.953913Z\",\"shell.execute_reply\":\"2022-11-26T22:05:57.327821Z\"}}\nimportances_most = importances.head(10) # 10 most important features\naxes = sns.factorplot('feature','importance', \n                      data=importances_most, aspect = 4, )\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:57.330866Z\",\"iopub.execute_input\":\"2022-11-26T22:05:57.331743Z\",\"iopub.status.idle\":\"2022-11-26T22:05:57.740401Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:57.331614Z\",\"shell.execute_reply\":\"2022-11-26T22:05:57.739219Z\"}}\nimportances_least = importances.tail(10) # least 10 important features\naxes = sns.factorplot('feature','importance', \n                      data=importances_least, aspect = 4,)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:57.742354Z\",\"iopub.execute_input\":\"2022-11-26T22:05:57.743126Z\",\"iopub.status.idle\":\"2022-11-26T22:05:58.002046Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:57.743011Z\",\"shell.execute_reply\":\"2022-11-26T22:05:58.001012Z\"}}\n# Random Forest , Testing with oob score\n\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")\n\n# %% [markdown]\n# # OOB Score\n\n# %% [markdown]\n# Our random forest model predicts as good as it did before. A general rule is that, the more features you have, the more likely your model will suffer from overfitting and vice versa. But I think our data looks fine for now and hasn't too much features.\n# \n# There is also another way to evaluate a random-forest classifier, which is probably much more accurate than the score we used before. What I am talking about is the out-of-bag samples to estimate the generalization accuracy. I will not go into details here about how it works. Just note that out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:58.003504Z\",\"iopub.execute_input\":\"2022-11-26T22:05:58.004189Z\",\"iopub.status.idle\":\"2022-11-26T22:05:58.010024Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:58.004122Z\",\"shell.execute_reply\":\"2022-11-26T22:05:58.008972Z\"}}\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")\n\n# %% [markdown]\n# # Hyperparameter Tuning\n# Below you can see the code of the hyperparamter tuning for the parameters criterion, min_samples_leaf, min_samples_split and n_estimators.\n# \n# I put this code into a markdown cell and not into a code cell, because it takes a long time to run it. Directly underneeth it, I put a screenshot of the gridsearch's output.\n# \n# param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n# \n# from sklearn.model_selection import GridSearchCV, cross_val_score\n# \n# rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n# \n# clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n# \n# clf.fit(X_train, Y_train)\n# \n# clf.bestparams\n\n# %% [markdown]\n# # Testing new parameters from hypertuning\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:58.011140Z\",\"iopub.execute_input\":\"2022-11-26T22:05:58.011495Z\",\"iopub.status.idle\":\"2022-11-26T22:05:58.413090Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:58.011434Z\",\"shell.execute_reply\":\"2022-11-26T22:05:58.412275Z\"}}\n# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=32, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")\n\n# %% [markdown]\n# Now that we have a proper model, we can start evaluating it's performace in a more accurate way. Previously we only used accuracy and the oob score, which is just another form of accuracy. The problem is just, that it's more complicated to evaluate a classification model than a regression model. We will talk about this in the following section.\n\n# %% [markdown]\n# # Confusion Matrix\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:58.414753Z\",\"iopub.execute_input\":\"2022-11-26T22:05:58.415155Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.246661Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:58.415077Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.245817Z\"}}\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)\n\n# %% [markdown]\n# The first row is about the not-survived-predictions: 489 passengers were correctly classified as not survived (called true negatives) and 60 were wrongly classified as not survived (false positives).\n# \n# The second row is about the survived-predictions: 100 passengers where wrongly classified as survived (false negatives) and 242 were correctly classified as survived (true positives).\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.247793Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.248051Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.302814Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.248006Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.300579Z\"}}\nconf_mat = confusion_matrix(Y_train, predictions)\nTP = conf_mat[0][0]\nFP = conf_mat[0][1]\nFN = conf_mat[1][0]\nTN = conf_mat[1][1]\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\nprint('Sensitivity, hit rate, recall, or true positive rate=',TPR)\n\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \nprint('Specificity or true negative rate=',TNR)\n\n# Precision or positive predictive value\nPPV = TP/(TP+FP)\nprint('Precision or positive predictive value=',PPV)\n\n# Negative predictive value\nNPV = TN/(TN+FN)\nprint('Negative predictive value=',NPV)\n\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\nprint('Fall out or false positive rate=',FPR)\n\n# False negative rate\nFNR = FN/(TP+FN)\nprint('False negative rate=',FNR)\n\n# False discovery rate\nFDR = FP/(TP+FP)\nprint('False discovery rate=',FDR)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nprint('Overall accuracy=',ACC)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.304402Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.304631Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.312493Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.304595Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.311166Z\"}}\npositives = pd.DataFrame({\n    'Factor': ['True Positives', 'False Positives', ],\n    'Score': [TP, FP]})\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.314105Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.314638Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.508101Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.314535Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.506915Z\"}}\nsns.barplot(x='Factor',y='Score',data=positives)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.510176Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.510932Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.520459Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.510820Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.519149Z\"}}\nnegatives = pd.DataFrame({\n    'Factor':['True Negative', 'False Negative'],\n    'Score':[TN, FN]\n})\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.522623Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.523375Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.731370Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.523293Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.730061Z\"}}\nsns.barplot(x='Factor',y='Score',data=negatives)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.733615Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.734382Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.749025Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.734294Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.747784Z\"}}\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))\n\n# %% [markdown]\n# Our model predicts 80% of the time, a passengers survival correctly (precision). \n# The recall tells us that it predicted the survival of 71 % of the people who actually survived.\n\n# %% [markdown]\n# # F-Score\n\n# %% [markdown]\n# You can combine precision and recall into one score, which is called the F-score. The F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. \n# As a result of that, the classifier will only get a high F-score, if both recall and precision are high.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.751210Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.751980Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.764775Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.751887Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.762980Z\"}}\nfrom sklearn.metrics import f1_score\nprint('F1score',f1_score(Y_train, predictions))\n\n\n# %% [markdown]\n# There we have it, a 75 % F-score. The score is not that high, because we have a recall of 70%.\n# \n# But unfortunately the F-score is not perfect, because it favors classifiers that have a similar precision and recall. \n# This is a problem, because you sometimes want a high precision and sometimes a high recall. \n# The thing is that an increasing precision, sometimes results in an decreasing recall and vice versa (depending on the threshold). \n# This is called the precision/recall tradeoff. We will discuss this in the following section.\n\n# %% [markdown]\n# # Precision Recall Curve\n\n# %% [markdown]\n# For each person the Random Forest algorithm has to classify, it computes a probability based on a function \n# and it classifies the person as survived (when the score is bigger the than threshold) or \n# as not survived (when the score is smaller than the threshold). \n# That's why the threshold plays an important part.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.767631Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.768449Z\",\"iopub.status.idle\":\"2022-11-26T22:05:59.884560Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.768356Z\",\"shell.execute_reply\":\"2022-11-26T22:05:59.883637Z\"}}\nfrom sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:05:59.885814Z\",\"iopub.execute_input\":\"2022-11-26T22:05:59.886092Z\",\"iopub.status.idle\":\"2022-11-26T22:06:00.205451Z\",\"shell.execute_reply.started\":\"2022-11-26T22:05:59.886045Z\",\"shell.execute_reply\":\"2022-11-26T22:06:00.204195Z\"}}\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()\n\n# %% [markdown]\n# Above you can clearly see that the recall is falling of rapidly at a precision of around 84%. \n# Because of that you may want to select the precision/recall tradeoff before that - maybe at around 75 %.\n# \n# You are now able to choose a threshold, that gives you the best precision/recall tradeoff \n# for your current machine learning problem. If you want for example a precision of 80%, \n# you can easily look at the plots and see that you would need a threshold of around 0.4. \n# Then you could train a model with exactly that threshold and would get the desired accuracy.\n# \n# Another way is to plot the precision and recall against each other\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:06:00.207952Z\",\"iopub.execute_input\":\"2022-11-26T22:06:00.208865Z\",\"iopub.status.idle\":\"2022-11-26T22:06:00.511373Z\",\"shell.execute_reply.started\":\"2022-11-26T22:06:00.208755Z\",\"shell.execute_reply\":\"2022-11-26T22:06:00.510176Z\"}}\ndef plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()\n\n# %% [markdown]\n# # ROC AUC Curve\n\n# %% [markdown]\n# Another way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. \n# This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:06:00.513699Z\",\"iopub.execute_input\":\"2022-11-26T22:06:00.514584Z\",\"iopub.status.idle\":\"2022-11-26T22:06:00.523850Z\",\"shell.execute_reply.started\":\"2022-11-26T22:06:00.514492Z\",\"shell.execute_reply\":\"2022-11-26T22:06:00.522428Z\"}}\nfrom sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:06:00.526273Z\",\"iopub.execute_input\":\"2022-11-26T22:06:00.526604Z\",\"iopub.status.idle\":\"2022-11-26T22:06:00.830770Z\",\"shell.execute_reply.started\":\"2022-11-26T22:06:00.526533Z\",\"shell.execute_reply\":\"2022-11-26T22:06:00.829514Z\"}}\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()\n\n# %% [markdown]\n# The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Our Random Forest model seems to do a good job.\n# \n# Of course we also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is.\n\n# %% [markdown]\n# # ROC AUC Score\n\n# %% [markdown]\n# The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.\n# \n# A classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:06:00.833066Z\",\"iopub.execute_input\":\"2022-11-26T22:06:00.834175Z\",\"iopub.status.idle\":\"2022-11-26T22:06:00.845181Z\",\"shell.execute_reply.started\":\"2022-11-26T22:06:00.834019Z\",\"shell.execute_reply\":\"2022-11-26T22:06:00.844495Z\"}}\nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)\n\n# %% [markdown]\n# **Submission**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2022-11-26T22:06:00.846339Z\",\"iopub.execute_input\":\"2022-11-26T22:06:00.846782Z\",\"iopub.status.idle\":\"2022-11-26T22:06:01.042695Z\",\"shell.execute_reply.started\":\"2022-11-26T22:06:00.846733Z\",\"shell.execute_reply\":\"2022-11-26T22:06:01.041945Z\"}}\nsubmission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)\n\n# %% [code]\n**More to come...\nPlease provide suggestions if there is need of improvement.\nPlease upvote if the kernel was useful**","metadata":{"_uuid":"7ea2cd98-2264-4315-b291-41d87b201249","_cell_guid":"db5a7fb6-166a-4c0b-ae31-a2aeb0934ac8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}